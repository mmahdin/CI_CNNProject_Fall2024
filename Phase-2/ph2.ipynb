{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from os import listdir\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CyclicLR\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "from ph2 import *\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./dataset/archive/\"\n",
    "folder = listdir(base_path)\n",
    "len(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 280 patients. That's a small number compared to the expected number of patients one would like to analyse with our algorithm after deployment. **Consequently overfitting to this specific patient distribution is very likely and we need to take care about the generalization performance of our model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many patches do we have in total?\n",
    "\n",
    "Our algorithm needs to decide whether an image patch contains IDC or not. Consequently not the whole patient tissue slice but the single patches have to be considered as input to our algorithm. How many of them do we have in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = 0\n",
    "for n in range(len(folder)):\n",
    "    patient_id = folder[n]\n",
    "    for c in [0, 1]:\n",
    "        patient_path = base_path + patient_id \n",
    "        class_path = patient_path + \"/\" + str(c) + \"/\"\n",
    "        subfiles = listdir(class_path)\n",
    "        total_images += len(subfiles)\n",
    "total_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, roughly 280000 images. To feed the algorithm with image patches it would be nice to store the path of each image. This way we can load batches of images only one by one without storing the pixel values of all images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the image_path, patient_id and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(index=np.arange(0, total_images), columns=[\"patient_id\", \"path\", \"target\"])\n",
    "\n",
    "k = 0\n",
    "for n in range(len(folder)):\n",
    "    patient_id = folder[n]\n",
    "    patient_path = base_path + patient_id \n",
    "    for c in [0,1]:\n",
    "        class_path = patient_path + \"/\" + str(c) + \"/\"\n",
    "        subfiles = listdir(class_path)\n",
    "        for m in range(len(subfiles)):\n",
    "            image_path = subfiles[m]\n",
    "            data.iloc[k][\"path\"] = class_path + image_path\n",
    "            data.iloc[k][\"target\"] = c\n",
    "            data.iloc[k][\"patient_id\"] = patient_id\n",
    "            k += 1  \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis <a class=\"anchor\" id=\"eda\"></a>\n",
    "\n",
    "## What do we know about our data? <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cancer percentage per patient\n",
    "cancer_perc = data.groupby(\"patient_id\")[\"target\"].value_counts(normalize=True).unstack()\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Plot the distribution of the number of patches per patient\n",
    "sns.histplot(data.groupby(\"patient_id\").size(), ax=ax[0], color=\"orange\", bins=30)\n",
    "ax[0].set_xlabel(\"Number of patches\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_title(\"How many patches do we have per patient?\")\n",
    "\n",
    "# Plot the percentage of patches with IDC\n",
    "sns.histplot(cancer_perc.loc[:, 1] * 100, ax=ax[1], color=\"tomato\", bins=30)\n",
    "ax[1].set_title(\"How much percentage of an image is covered by IDC?\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "ax[1].set_xlabel(\"% of patches with IDC\")\n",
    "\n",
    "# Plot the count of patches with and without IDC\n",
    "sns.countplot(x=data[\"target\"], hue=data[\"target\"], palette=\"Set2\", ax=ax[2], legend=False)\n",
    "ax[2].set_xlabel(\"No (0) versus Yes (1)\")\n",
    "ax[2].set_title(\"How many patches show IDC?\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "1. The number of image patches per patient varies a lot! **This leads to the questions whether all images show the same resolution of tissue cells of if this varies between patients**. \n",
    "2. Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer. **Does a tissue slice per patient cover the whole region of interest?**\n",
    "3. The **classes of IDC versus no IDC are imbalanced**. We have to check this again after setting up a validation strategy and find a strategy to deal with class weights (if we like to apply them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at healthy and cancer patches <a class=\"anchor\" id=\"patches\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target = data.target.astype(int)\n",
    "pos_selection = np.random.choice(data[data.target==1].index.values, size=50, replace=False)\n",
    "neg_selection = np.random.choice(data[data.target==0].index.values, size=50, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(5, 10, figsize=(20, 10))\n",
    "\n",
    "for n in range(5):\n",
    "    for m in range(10):\n",
    "        idx = pos_selection[m + 10 * n]  # Get the index of the image\n",
    "        image = mpimg.imread(data.loc[idx, \"path\"])  # Read the image\n",
    "        ax[n, m].imshow(image)  # Display the image\n",
    "        ax[n, m].axis(\"off\")  # Turn off axes for a cleaner look\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Healthy patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(5, 10, figsize=(20, 10))\n",
    "\n",
    "for n in range(5):\n",
    "    for m in range(10):\n",
    "        idx = neg_selection[m + 10 * n]  # Get the index of the image\n",
    "        image = mpimg.imread(data.loc[idx, \"path\"])  # Read the image\n",
    "        ax[n, m].imshow(image)  # Display the image\n",
    "        ax[n, m].axis(\"off\")  # Turn off axes for a cleaner look\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Sometimes we can find artifacts or incomplete patches that have smaller size than 50x50 pixels. \n",
    "* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n",
    "* Though some of the healthy patches are very violet colored too!\n",
    "* Would be very interesting to hear what criteria are important for a pathologist.\n",
    "* I assume that the wholes in the tissue belong to the mammary ducts where the milk can flow through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the breast tissue <a class=\"anchor\" id=\"tissue\"></a>\n",
    "\n",
    "This part is a bit tricky! We have to extract all coordinates of image patches that are stored in the image names. Then we can use the coordinates to reconstruct the whole breast tissue of a patient. This way we can also explore how diseased tissue looks like compared to healthy ones. To simplify this task let's write a method that takes a patient and outcomes a dataframe with coordinates and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coords(df, column=\"path\"):\n",
    "    \"\"\"Extract x and y coordinates from the file path.\"\"\"\n",
    "    coords = df[column].str.extract(r\"x(?P<x>\\d+)_y(?P<y>\\d+)\")\n",
    "    df[\"x\"] = coords[\"x\"].astype(int)\n",
    "    df[\"y\"] = coords[\"y\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def get_cancer_dataframe(patient_id, cancer_id):\n",
    "    \"\"\"Create a DataFrame for a specific cancer ID.\"\"\"\n",
    "    path = f\"{base_path}/{patient_id}/{cancer_id}\"\n",
    "    files = listdir(path)\n",
    "    \n",
    "    dataframe = pd.DataFrame(files, columns=[\"path\"])\n",
    "    dataframe[\"path\"] = path + \"/\" + dataframe[\"path\"]\n",
    "    dataframe[\"target\"] = int(cancer_id)\n",
    "    \n",
    "    # Extract x and y coordinates\n",
    "    dataframe = extract_coords(dataframe, column=\"path\")\n",
    "    return dataframe\n",
    "\n",
    "def get_patient_dataframe(patient_id):\n",
    "    \"\"\"Combine DataFrames for both cancer ID 0 and 1 for a patient.\"\"\"\n",
    "    df_0 = get_cancer_dataframe(patient_id, \"0\")\n",
    "    df_1 = get_cancer_dataframe(patient_id, \"1\")\n",
    "    \n",
    "    # Combine dataframes\n",
    "    patient_df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "    return patient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = get_patient_dataframe(data.patient_id.values[0])\n",
    "example.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have the coordinates for each patch, its path to load the image and its target information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary target visualisation per tissue slice <a class=\"anchor\" id=\"binarytissue\"></a>\n",
    "\n",
    "Before we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,3,figsize=(20, 27))\n",
    "\n",
    "patient_ids = data.patient_id.unique()\n",
    "\n",
    "for n in range(5):\n",
    "    for m in range(3):\n",
    "        patient_id = patient_ids[m + 3*n]\n",
    "        example_df = get_patient_dataframe(patient_id)\n",
    "        \n",
    "        ax[n,m].scatter(example_df.x.values, example_df.y.values, c=example_df.target.values, cmap=\"coolwarm\", s=20);\n",
    "        ax[n,m].set_title(\"patient \" + patient_id)\n",
    "        ax[n,m].set_xlabel(\"y coord\")\n",
    "        ax[n,m].set_ylabel(\"x coord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation. \n",
    "* Reading the paper (link!) that seems to be related to this data this could also be part of the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the breast tissue images <a class=\"anchor\" id=\"tissueimages\"></a>\n",
    "\n",
    "Ok, now it's time to go one step deeper with our EDA. Given the coordinates of image patches we could try to reconstruct the whole tissue image (not only the targets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_breast_tissue(patient_id, pred_df=None):\n",
    "    \"\"\"Visualize breast tissue and generate grid and mask representations.\"\"\"\n",
    "    # Get the patient data\n",
    "    example_df = get_patient_dataframe(patient_id)\n",
    "    max_point = [example_df.y.max() - 1, example_df.x.max() - 1]\n",
    "    \n",
    "    # Initialize grid and masks\n",
    "    grid = 255 * np.ones(shape=(max_point[0] + 50, max_point[1] + 50, 3))\n",
    "    mask = 255 * np.ones(shape=(max_point[0] + 50, max_point[1] + 50, 3))\n",
    "    mask_proba = np.zeros(shape=(max_point[0] + 50, max_point[1] + 50, 1))\n",
    "    \n",
    "    if pred_df is not None:\n",
    "        patient_df = pred_df[pred_df.patient_id == patient_id].copy()\n",
    "    \n",
    "    broken_patches = []  # To store paths of images that cause issues\n",
    "    \n",
    "    # Process each patch\n",
    "    for n in range(len(example_df)):\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = mpimg.imread(example_df.path.values[n])\n",
    "            \n",
    "            target = example_df.target.values[n]\n",
    "            \n",
    "            # Get coordinates\n",
    "            x_coord = int(example_df.x.values[n])\n",
    "            y_coord = int(example_df.y.values[n])\n",
    "            x_start = x_coord - 1\n",
    "            y_start = y_coord - 1\n",
    "            x_end = x_start + 50\n",
    "            y_end = y_start + 50\n",
    "\n",
    "            # Place the image on the grid\n",
    "            grid[y_start:y_end, x_start:x_end] = image\n",
    "            \n",
    "            # Update mask for target 1\n",
    "            if target == 1:\n",
    "                mask[y_start:y_end, x_start:x_end, 0] = 250  # Red\n",
    "                mask[y_start:y_end, x_start:x_end, 1] = 0    # Green\n",
    "                mask[y_start:y_end, x_start:x_end, 2] = 0    # Blue\n",
    "            \n",
    "            # Update probability mask if prediction DataFrame is provided\n",
    "            if pred_df is not None:\n",
    "                proba = patient_df[\n",
    "                    (patient_df.x == x_coord) & (patient_df.y == y_coord)\n",
    "                ].proba\n",
    "                mask_proba[y_start:y_end, x_start:x_end, 0] = float(proba)\n",
    "\n",
    "        except ValueError:\n",
    "            # Catch errors and add to broken patches list\n",
    "            broken_patches.append(example_df.path.values[n])\n",
    "    \n",
    "    return grid, mask, broken_patches, mask_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with patient ID\n",
    "example = \"13616\"  # Patient ID to visualize\n",
    "grid, mask, broken_patches, _ = visualise_breast_tissue(example)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Display the grid and mask\n",
    "ax[0].imshow(grid, alpha=0.9)\n",
    "ax[1].imshow(mask, alpha=0.8)\n",
    "ax[1].imshow(grid, alpha=0.7)  # Overlay grid on the mask\n",
    "\n",
    "# Remove grids for both axes\n",
    "for axis in ax:\n",
    "    axis.grid(False)\n",
    "\n",
    "# Set labels and titles for both subplots\n",
    "for i, title in enumerate([\"Breast tissue slice\", \"Cancer tissue colored red\"]):\n",
    "    ax[i].set_xlabel(\"x-coordinate\")\n",
    "    ax[i].set_ylabel(\"y-coordinate\")\n",
    "    ax[i].set_title(f\"{title} of patient: {example}\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* The tissue on the left is shown without target information.\n",
    "* The image on the right shows the same tissue but cancer is stained with intensive red color. \n",
    "* Comparing both images it seems that darker, more violet colored tissue has a higher chance to be cancer than those with rose color. \n",
    "* But as one can see it's not always the case. So we need to ask ourselves if violet tissue patches have more mammary ducts than rose ones. If this is true we have to be careful. Our model might start to learn that mammary ducts are always related to cancer! \n",
    "\n",
    "Sometimes it's not possible to load an image patch as the path is ill defined. But in our case, we were able to load them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the machine learning workflow <a class=\"anchor\" id=\"workflow\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation strategy <a class=\"anchor\" id=\"validation\"></a>\n",
    "\n",
    "Let's start very simple by selecting 30 % of the patients as test data and the remaining 70 % for training and developing. This seems arbitrary and we should rethink this strategy in the next cycle of our datascience workflow. \n",
    "\n",
    "A better idea could be to cluster patients with dependence on the size of the tumor, the number of total patches and statistical quantities of area coverd by the patches. The reason behind that is that we would like to have test patients that cover a broad range of possible variations. Only then we can measure something like a generalisation performance.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = data.patient_id.unique()\n",
    "\n",
    "train_ids, sub_test_ids = train_test_split(patients,\n",
    "                                           test_size=0.3,\n",
    "                                           random_state=0)\n",
    "test_ids, dev_ids = train_test_split(sub_test_ids, test_size=0.5, random_state=0)\n",
    "\n",
    "print(len(train_ids), len(dev_ids), len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's 70 % train and 15 % for dev and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.loc[data.patient_id.isin(train_ids),:].copy()\n",
    "test_df = data.loc[data.patient_id.isin(test_ids),:].copy()\n",
    "dev_df = data.loc[data.patient_id.isin(dev_ids),:].copy()\n",
    "\n",
    "train_df = extract_coords(train_df)\n",
    "test_df = extract_coords(test_df)\n",
    "dev_df = extract_coords(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target distributions <a class=\"anchor\" id=\"target_dists\"></a>\n",
    "\n",
    "Let's take a look at the target distribution difference of the datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Plot for train data\n",
    "sns.histplot(data=train_df, x=\"target\", ax=ax[0], color=\"red\", discrete=True)\n",
    "ax[0].set_title(\"Train Data\")\n",
    "ax[0].set_xlabel(\"Target\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot for dev data\n",
    "sns.histplot(data=dev_df, x=\"target\", ax=ax[1], color=\"blue\", discrete=True)\n",
    "ax[1].set_title(\"Dev Data\")\n",
    "ax[1].set_xlabel(\"Target\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot for test data\n",
    "sns.histplot(data=test_df, x=\"target\", ax=ax[2], color=\"green\", discrete=True)\n",
    "ax[2].set_title(\"Test Data\")\n",
    "ax[2].set_xlabel(\"Target\")\n",
    "ax[2].set_ylabel(\"Count\")\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test data has more cancer patches compared to healthy tissue patches than train or dev. We should keep this in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"./dataset/augmented/\" \n",
    "\n",
    "# Define source and destination directories\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# Define Albumentations augmentation pipeline\n",
    "# Define Albumentations augmentation pipeline\n",
    "augmentation_pipeline = A.Compose([\n",
    "    # Slightly adjust brightness and contrast to maintain medical information\n",
    "    # A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "    # Elastic Transform: Reduce distortion to avoid excessive deformation of features\n",
    "    # A.ElasticTransform(alpha=30, sigma=30 * 0.05, alpha_affine=30 * 0.03, p=0.5),\n",
    "    # Random Resized Crop: Ensure the crop covers most of the important tissue area\n",
    "    A.RandomResizedCrop(height=50, width=50, scale=(0.9, 1.0), p=0.5),\n",
    "    # CoarseDropout: Reduce the size and number of holes to avoid losing crucial regions\n",
    "    # A.CoarseDropout(max_holes=3, max_height=3, max_width=3, p=0.5),\n",
    "    # Horizontal Flip: Augment by flipping images horizontally\n",
    "    A.HorizontalFlip(p=0.2),\n",
    "    # Vertical Flip: Augment by flipping images vertically\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    # Resize to maintain consistency\n",
    "    A.Resize(height=50, width=50)\n",
    "])\n",
    "\n",
    "\n",
    "# Function to apply augmentations\n",
    "def augment_image_albumentations(image):\n",
    "    # Convert PIL image to numpy array\n",
    "    image_np = np.array(image)\n",
    "    augmented = augmentation_pipeline(image=image_np)\n",
    "    return Image.fromarray(augmented[\"image\"])\n",
    "\n",
    "# Find class distributions\n",
    "class_counts = train_df['target'].value_counts()\n",
    "minority_class = class_counts.idxmin()  # Class with fewer samples\n",
    "majority_class = class_counts.idxmax()  # Class with more samples\n",
    "\n",
    "# Filter rows for minority class\n",
    "minority_data = train_df[train_df['target'] == minority_class]\n",
    "majority_count = class_counts[majority_class]\n",
    "\n",
    "# List to store new rows for the augmented dataset\n",
    "new_rows = []\n",
    "\n",
    "# Perform augmentation until the dataset is balanced\n",
    "while len(minority_data) + len(new_rows) < majority_count:\n",
    "    for index, row in minority_data.iterrows():\n",
    "        # Check if balance is achieved\n",
    "        if len(minority_data) + len(new_rows) >= majority_count:\n",
    "            break\n",
    "        \n",
    "        image_path = row['path']\n",
    "        patient_id = row['patient_id']\n",
    "        x, y = row['x'], row['y']\n",
    "\n",
    "        # Apply augmentations\n",
    "        new_image_name = os.path.basename(\"aug_\" + image_path.split('/')[-1])  # Augmented name\n",
    "        new_image_path = os.path.join(dest_dir, new_image_name)\n",
    "        \n",
    "        # Skip if already augmented\n",
    "        if os.path.exists(new_image_path):\n",
    "            new_rows.append({\n",
    "                'patient_id': patient_id,\n",
    "                'path': new_image_path,\n",
    "                'target': minority_class,\n",
    "                'x': x,\n",
    "                'y': y\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Augment image using Albumentations\n",
    "        augmented_image = augment_image_albumentations(image)\n",
    "        augmented_image.save(new_image_path)\n",
    "\n",
    "        # Add new row to the dataset\n",
    "        new_rows.append({\n",
    "            'patient_id': patient_id,\n",
    "            'path': new_image_path,\n",
    "            'target': minority_class,\n",
    "            'x': x,\n",
    "            'y': y\n",
    "        })\n",
    "\n",
    "    if len(minority_data) + len(new_rows) >= majority_count:\n",
    "        break\n",
    "\n",
    "# Append new rows to the dataset\n",
    "train_df = pd.concat([train_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Check class distribution after augmentation\n",
    "print(f\"Class distribution after augmentation:\\n{train_df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cache = {}\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    image_path = row[\"path\"]\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_cache[image_path] = image\n",
    " \n",
    "dest_dir = \"./dataset/augmented/\" \n",
    "augmented = listdir(dest_dir)\n",
    "\n",
    "for i in augmented:\n",
    "    image_path = os.path.join(dest_dir, i)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_cache[image_path] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Plot for train data\n",
    "sns.histplot(data=train_df, x=\"target\", ax=ax[0], color=\"red\", discrete=True)\n",
    "ax[0].set_title(\"Train Data\")\n",
    "ax[0].set_xlabel(\"Target\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot for dev data\n",
    "sns.histplot(data=dev_df, x=\"target\", ax=ax[1], color=\"blue\", discrete=True)\n",
    "ax[1].set_title(\"Dev Data\")\n",
    "ax[1].set_xlabel(\"Target\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot for test data\n",
    "sns.histplot(data=test_df, x=\"target\", ax=ax[2], color=\"green\", discrete=True)\n",
    "ax[2].set_title(\"Test Data\")\n",
    "ax[2].set_xlabel(\"Target\")\n",
    "ax[2].set_ylabel(\"Count\")\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pytorch image datasets <a class=\"anchor\" id=\"image_datasets\"></a>\n",
    "\n",
    "It's often a good idea to start as simple as possible and to grow more complex while iterating through the solution. This way we prevent to build up models that are likely overfitted to the available data and we can find out useful ideas in a strategic manner instead of trying out every idea at random.\n",
    "\n",
    "The simplest transformations we can do for each image are:\n",
    "\n",
    "* resizing the images to the desired input shape\n",
    "* performing horizontal and vertical flips\n",
    "\n",
    "In our case the patches are of shape 50x50x3 and we could set this as our input shape. As CNNs are translational but not rotational invariant, it's a good idea to add flips during training. This way we increase the variety of our data in a meaningful way as each patch could be rotated as well on the tissue slice. As we are not looking at the whole tissue we are not loosing spatial connections between patches and it's not important that some neighboring patches are rotated in different directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((50, 50)),  # Resize all images to 50x50\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# train_dataset = BreastCancerDataset(train_df, transform=transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dtype=torch.float32\n",
    "# mean = 0.0\n",
    "# std = 0.0\n",
    "# nb_samples = 0.0\n",
    "# for batch_idx, batch in enumerate(train_dataloader):\n",
    "#     x = batch[\"image\"].to(device=device, dtype=dtype)\n",
    "#     y = batch[\"label\"].to(device=device)\n",
    "#     batch_samples = x.size(0)\n",
    "#     data = x.view(batch_samples, x.size(1), -1)\n",
    "#     mean += data.mean(2).sum(0)\n",
    "#     std += data.std(2).sum(0)\n",
    "#     nb_samples += batch_samples\n",
    "\n",
    "# mean /= nb_samples\n",
    "# std /= nb_samples\n",
    "# print(f'Mean: {mean}')\n",
    "# print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transform(key=\"train\", plot=False):\n",
    "    train_sequence = [\n",
    "        transforms.Resize((50, 50)),\n",
    "        # Data Augmentations for Training\n",
    "        transforms.RandomHorizontalFlip(p=0.1),  # Horizontal flip\n",
    "        transforms.RandomRotation(10),  # Reduced rotation to ±15 degrees\n",
    "        transforms.RandomResizedCrop(size=(50, 50), scale=(0.95, 1.0)),  # Slightly reduced cropping scale\n",
    "    ]\n",
    "    \n",
    "    val_sequence = [\n",
    "        transforms.Resize((50, 50))  # Only resizing for validation\n",
    "    ]\n",
    "    \n",
    "    # Convert to tensor and normalize for both train and validation\n",
    "    if not plot:\n",
    "        train_sequence.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.7854, 0.6031, 0.7135], [0.0953, 0.1400, 0.1035])  # Normalize to ImageNet stats\n",
    "        ])\n",
    "        val_sequence.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.7854, 0.6031, 0.7135], [0.0953, 0.1400, 0.1035])  # Normalize to ImageNet stats\n",
    "        ])\n",
    "    \n",
    "    # Define transformations for train and val\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose(train_sequence),\n",
    "        'val': transforms.Compose(val_sequence)\n",
    "    }\n",
    "    \n",
    "    return data_transforms[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore we need to create a dataset that loads an image patch of a patient, converts it to RGB, performs the augmentation if it's desired and returns the image, the target, the patient id and the image coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Dataset class to use the image cache\n",
    "class BreastCancerDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.states = df\n",
    "        self.transform = transform\n",
    "        self.image_cache = image_cache  # Use the preloaded images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.states.patient_id.values[idx]\n",
    "        x_coord = self.states.x.values[idx]\n",
    "        y_coord = self.states.y.values[idx]\n",
    "        image_path = self.states.path.values[idx]\n",
    "        \n",
    "        # Retrieve the preloaded image\n",
    "        image = self.image_cache[image_path]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if \"target\" in self.states.columns.values:\n",
    "            target = int(self.states.target.values[idx])\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": target,\n",
    "            \"patient_id\": patient_id,\n",
    "            \"x\": x_coord,\n",
    "            \"y\": y_coord\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BreastCancerDataset(train_df, transform=my_transform(key=\"train\"))\n",
    "dev_dataset = BreastCancerDataset(dev_df, transform=my_transform(key=\"val\"))\n",
    "test_dataset = BreastCancerDataset(test_df, transform=my_transform(key=\"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\"train\": train_dataset, \"dev\": dev_dataset, \"test\": test_dataset}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"dev\", \"test\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 6, figsize=(20, 11))\n",
    "\n",
    "# Ensure transforms return images that can be visualized\n",
    "train_transform = my_transform(key=\"train\", plot=True)\n",
    "val_transform = my_transform(key=\"val\", plot=True)\n",
    "\n",
    "for m in range(6):\n",
    "    filepath = train_df.path.values[m]\n",
    "    \n",
    "    # Open the image using PIL\n",
    "    image = Image.open(filepath)\n",
    "    ax[0, m].imshow(image)\n",
    "    ax[0, m].grid(False)\n",
    "    ax[0, m].set_title(\n",
    "        f\"{train_df.patient_id.values[m]}\\nTarget: {train_df.target.values[m]}\"\n",
    "    )\n",
    "    \n",
    "    # Apply training transformation\n",
    "    transformed_train_img = train_transform(image)\n",
    "    \n",
    "    # Convert transformed image to numpy array for visualization\n",
    "    if isinstance(transformed_train_img, torch.Tensor):\n",
    "        transformed_train_img = transformed_train_img.permute(1, 2, 0).numpy()  # Convert CxHxW to HxWxC\n",
    "    ax[1, m].imshow(transformed_train_img)\n",
    "    ax[1, m].grid(False)\n",
    "    ax[1, m].set_title(\"Preprocessing for train\")\n",
    "    \n",
    "    # Apply validation transformation\n",
    "    transformed_val_img = val_transform(image)\n",
    "    \n",
    "    # Convert transformed image to numpy array for visualization\n",
    "    if isinstance(transformed_val_img, torch.Tensor):\n",
    "        transformed_val_img = transformed_val_img.permute(1, 2, 0).numpy()  # Convert CxHxW to HxWxC\n",
    "    ax[2, m].imshow(transformed_val_img)\n",
    "    ax[2, m].grid(False)\n",
    "    ax[2, m].set_title(\"Preprocessing for val\")\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation we have only used the image resizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pytorch dataloaders <a class=\"anchor\" id=\"dataloaders\"></a>\n",
    "\n",
    "As the gradients for each learning step are computed over batches we benefit from shuffling the training data after each epoch. This way each batch is composed differently and we don't start to learn for specific sequences of images. For validation and training we drop the last batch that often consists less images than the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_dataloader, \"dev\": dev_dataloader, \"test\": test_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataloaders[\"train\"]), len(dataloaders[\"dev\"]), len(dataloaders[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet structure for your task\n",
    "networks = {\n",
    "    'best': {\n",
    "        'block': ResidualBlock,\n",
    "        'stage_args': [\n",
    "            (32, 64, 3, False),\n",
    "            (64, 128, 2, True)\n",
    "        ],\n",
    "        'dropout': True,  # Enable dropout\n",
    "        'p': 0.4  # Dropout probability\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_resnet(name):\n",
    "    return ResNet(**networks[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_float= torch.float\n",
    "to_long = torch.long\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.006\n",
    "weight_decay = 2e-4\n",
    "epochs = 50\n",
    "\n",
    "# Model and file paths\n",
    "name = 'best'\n",
    "version = 2\n",
    "checkpoint_path = f'./checkpoint/{name}_{version}_checkpoint.pth'\n",
    "model_path = f'./models/{name}_{version}_checkpoint.pth'\n",
    "history_path = f'./history/{name}_{version}.pth'\n",
    "\n",
    "min_lr = 1e-6\n",
    "max_lr = 0.006\n",
    "max_iterations = int(len(dataloaders[\"train\"])/2)\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Resume training from checkpoint\n",
    "    print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model = get_resnet(name).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    # optimizer = optim.SGD(model.fc.parameters(), min_lr)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "\n",
    "    # scheduler = CyclicLR(optimizer=optimizer,\n",
    "    #                      base_lr=min_lr,\n",
    "    #                      max_lr=max_lr,\n",
    "    #                      step_size_up=max_iterations,\n",
    "    #                      step_size_down=max_iterations,\n",
    "    #                      mode=\"triangular\")\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "    if checkpoint['scheduler_state'] is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_acc = checkpoint['best_val_acc']\n",
    "    train_metrics_history = checkpoint['train_history']\n",
    "    val_metrics_history = checkpoint['val_history']\n",
    "    lr_history = checkpoint['lr_history']\n",
    "\n",
    "    print(f\"Training will resume from epoch {start_epoch}.\\n\")\n",
    "\n",
    "else:\n",
    "    # Start new training\n",
    "    print(f\"Training new model: {name}\\n\")\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = get_resnet(name).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    # optimizer = optim.SGD(model.fc.parameters(), min_lr)\n",
    "\n",
    "    # Define scheduler\n",
    "    # scheduler = CyclicLR(optimizer=optimizer,\n",
    "    #                      base_lr=min_lr,\n",
    "    #                      max_lr=max_lr,\n",
    "    #                      step_size_up=max_iterations,\n",
    "    #                      step_size_down=max_iterations,\n",
    "    #                      mode=\"triangular\")\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "    # Initialize metrics and state\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    train_metrics_history = {'loss': [], 'accuracy': [],\n",
    "                             'precision': [], 'recall': [], 'f1': []}\n",
    "    val_metrics_history = {'accuracy': [],\n",
    "                           'precision': [], 'recall': [], 'f1': []}\n",
    "    lr_history = []\n",
    "\n",
    "# Train model\n",
    "train_metrics_history, val_metrics_history, lr_history = train_model(\n",
    "    model, optimizer, train_dataloader, dev_dataloader,\n",
    "    device=device, dtype=torch.float32, epochs=epochs,\n",
    "    scheduler=scheduler, verbose=True,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    history_path=history_path\n",
    ")\n",
    "\n",
    "# Save final model and history after training completes\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Final model saved at: {model_path}\")\n",
    "\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump((train_metrics_history, val_metrics_history, lr_history), f)\n",
    "print(f\"Training history saved at: {history_path}\")\n",
    "\n",
    "# Evaluate model on the test set\n",
    "test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(\n",
    "    test_dataloader, model, device=device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, \"\n",
    "      f\"Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plot_all_metrics(train_metrics_history, val_metrics_history)\n",
    "plot_learning_rate(lr_history)\n",
    "plot_loss(train_metrics_history['loss'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
