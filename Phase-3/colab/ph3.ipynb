{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb34g0L2DzP5",
        "outputId": "1ec74fb1-9cb1-47d1-9067-6a52a8a91a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path_8k = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "\n",
        "print(\"Path to dataset files:\", path_8k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPFyf8hlEzCv",
        "outputId": "0851c24d-5e78-42fa-c9b8-1c31ead709e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions.txt  Images\n"
          ]
        }
      ],
      "source": [
        "!ls /root/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPMGiAvjD2Zl",
        "outputId": "b9391b65-e52f-4cda-dafe-c503ea25a34d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/phase3'\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/phase3')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path_30k = kagglehub.dataset_download(\"hsankesara/flickr-image-dataset\")\n",
        "path_30k += '/flickr30k_images'\n",
        "print(\"Path to dataset files:\", path_30k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz5MQ9SQIall",
        "outputId": "8b6d2e03-50cc-4cf2-9ab2-f84e5620de97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/hsankesara/flickr-image-dataset/versions/1/flickr30k_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "# Define the source directory containing images\n",
        "image_path = f'{path_30k}/flickr30k_images/'  # Replace with your actual source directory path\n",
        "\n",
        "# Define the destination directory\n",
        "destination_path = './images'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# Get a list of all images in the directory\n",
        "all_images = [f for f in os.listdir(image_path) if os.path.isfile(os.path.join(image_path, f))]\n",
        "\n",
        "# Randomly select 5000 images\n",
        "selected_images = all_images[:5000]\n",
        "\n",
        "# Copy the selected images to the destination directory\n",
        "for image in selected_images:\n",
        "    shutil.copy(os.path.join(image_path, image), os.path.join(destination_path, image))\n",
        "\n",
        "print(f\"Successfully copied {len(selected_images)} images to '{destination_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcemjG9gLSJP",
        "outputId": "12565e5e-7567-4cf1-b5b7-6681e4737cdf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied 5000 images to './images'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder and caption file paths\n",
        "image_folder = destination_path\n",
        "caption_file_path = f\"{path_30k}/results.csv\"\n",
        "output_file_path = './captions.csv'\n",
        "\n",
        "# Get a set of image names from the folder\n",
        "image_names = set(os.listdir(image_folder))\n",
        "\n",
        "# Filter the caption file\n",
        "with open(caption_file_path, \"r\") as caption_file:\n",
        "    lines = caption_file.readlines()\n",
        "\n",
        "filtered_lines = []\n",
        "\n",
        "for line in lines:\n",
        "    image_name = line.split(\"|\")[0].strip()\n",
        "    if image_name in image_names:\n",
        "        filtered_lines.append(line)\n",
        "\n",
        "# Write the filtered lines to a new file\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    output_file.writelines(filtered_lines)\n",
        "\n",
        "print(f\"Filtered captions saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjI4fp58MQmO",
        "outputId": "9048bb9f-8255-4009-862b-1470f718f1fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered captions saved to ./captions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FR0aGYRVDq_m"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ux-1o_RiDq_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d21744-79e7-418e-bd1e-6cd894dab6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# for plotting\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "from ph3 import *\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxZ9vVZCDq_o",
        "outputId": "850d765b-b767-49e0-9306-f5641831781d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to go!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available:\n",
        "  print('Good to go!')\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')\n",
        "\n",
        "# data type and device for torch.tensor\n",
        "to_float = {'dtype': torch.float, 'device': 'cpu'}\n",
        "to_float_cuda = {'dtype': torch.float, 'device': 'cuda'}\n",
        "to_double = {'dtype': torch.double, 'device': 'cpu'}\n",
        "to_double_cuda = {'dtype': torch.double, 'device': 'cuda'}\n",
        "to_long = {'dtype': torch.long, 'device': 'cpu'}\n",
        "to_long_cuda = {'dtype': torch.long, 'device': 'cuda'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFZ3MlGLDq_o",
        "outputId": "56099fba-a29c-4e40-b968-78d1dafdcb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/phase3/ph3.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset = torch.load(file_path[flicker])\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "image_size = (299, 299)\n",
        "flicker = '30k'\n",
        "data_path = {'8k':f\"{path_8k}/Images/\", '30k': image_folder}\n",
        "captions_path = {'8k':f\"{path_8k}/captions.txt\", '30k': output_file_path}\n",
        "data_dict_path = {'8k':f\"{base_dir}/image_captioning_dataset.pt\", '30k':f\"{base_dir}/flicker30k.pt\"}\n",
        "\n",
        "data_dict = load_data(data_dict_path, captions_path, data_path, image_size, flicker=flicker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NheDXxrkDq_o"
      },
      "outputs": [],
      "source": [
        "num_train = len(data_dict['train_images'])\n",
        "num_val = len(data_dict['val_images'])\n",
        "print(f'num_train: {num_train}')\n",
        "print(f'num_val: {num_val}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ-8D3DUl84J"
      },
      "source": [
        "### function for data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jS4yxUDq_p"
      },
      "outputs": [],
      "source": [
        "ag_img = process_images(data_dict['train_images'][0], image_size=image_size, augment=True)\n",
        "for i in range(ag_img.shape[0]):\n",
        "    img = ag_img[i].permute(1, 2, 0).to(torch.int32)\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8mh2jjnDq_p"
      },
      "outputs": [],
      "source": [
        "data_dict['train_captions'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKxSaLuODq_p"
      },
      "outputs": [],
      "source": [
        "len(data_dict[\"vocab\"][\"idx_to_token\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrVV7pMbDq_p"
      },
      "source": [
        "We have 6,472 lists of captions.\n",
        "\n",
        "- Each list contains 5 captions.  \n",
        "- Each caption consists of 40 tokens (with padding).  \n",
        "- Each token is represented as a vector of size 8,908 (one-hot encoding).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJdYaQWADq_q"
      },
      "outputs": [],
      "source": [
        "PAD_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRRDR5ERDq_q"
      },
      "source": [
        "`start = 1`,\n",
        "`end = 2`,\n",
        "`pad = 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLVgzIO5Dq_q"
      },
      "outputs": [],
      "source": [
        "# Sample a minibatch and show the reshaped 112x112 images and captions\n",
        "image_num = 1375\n",
        "\n",
        "sample_images = data_dict['train_images'][image_num]\n",
        "sample_captions = data_dict['train_captions'][image_num]\n",
        "plt.imshow(process_images(sample_images, image_size=image_size).permute(1, 2, 0).to(torch.int32))\n",
        "plt.axis('off')\n",
        "caption_str = decode_captions(sample_captions, data_dict['vocab']['idx_to_token'])\n",
        "caption_str = \"\\n\".join(caption_str)\n",
        "plt.title(caption_str)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCvJ8bZfDq_r"
      },
      "source": [
        "### RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHqDbDGvDq_r"
      },
      "outputs": [],
      "source": [
        "# create the image captioning model\n",
        "rnn_model = CaptioningRNN(\n",
        "          cell_type='attention',\n",
        "          word_to_idx=data_dict['vocab']['token_to_idx'],\n",
        "          input_dim=1280,\n",
        "          hidden_dim=512,\n",
        "          wordvec_dim=256,\n",
        "          ignore_index=PAD_index,\n",
        "          **to_float_cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hj3oJRTDq_r"
      },
      "outputs": [],
      "source": [
        "# Model and file paths\n",
        "name = 'attention'\n",
        "version = 10\n",
        "checkpoint_path = f'{base_dir}/checkpoint/{name}_{version}_checkpoint.pth'\n",
        "model_path = f'{base_dir}/models/{name}_{version}_checkpoint.pth'\n",
        "history_path = f'{base_dir}/history/{name}_{version}.pth'\n",
        "\n",
        "lr = 2e-3\n",
        "epochs = 200\n",
        "lr_decay = 0.97\n",
        "batch_size=512\n",
        "weight_decay = 5e-4\n",
        "interval = 20\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    # Resume training from checkpoint\n",
        "    print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model = rnn_model.to(device)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "\n",
        "    # scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: lr_decay ** (epoch // interval))\n",
        "    # scheduler = CosineAnnealingLR(optimizer, T_max=240)\n",
        "    scheduler = None\n",
        "\n",
        "    if checkpoint['scheduler_state'] is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
        "\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    train_loss_history = checkpoint['train_loss_history']\n",
        "    val_loss_history = checkpoint['val_loss_history']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "    print(f\"Training will resume from epoch {start_epoch}.\\n\")\n",
        "\n",
        "else:\n",
        "    # Start new training\n",
        "    print(f\"Training new model: {name}\\n\")\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = rnn_model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # optimizer = optim.SGD(model.fc.parameters(), min_lr)\n",
        "\n",
        "    # Define scheduler\n",
        "    # scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: lr_decay ** (epoch // interval))\n",
        "    # scheduler = CosineAnnealingLR(optimizer, T_max=240)\n",
        "    scheduler = None\n",
        "\n",
        "    # Initialize metrics and state\n",
        "    start_epoch = 0\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "# Train model\n",
        "train_loss_history, val_loss_history = train_captioning_model(\n",
        "    rnn_model, optimizer, data_dict, device='cuda', dtype=torch.float32, lr=lr,\n",
        "    epochs=epochs, batch_size=batch_size, scheduler=scheduler, val_perc=1,\n",
        "    image_size=image_size, verbose=True, checkpoint_path=checkpoint_path\n",
        ")\n",
        "\n",
        "# Save final model and history after training completes\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Final model saved at: {model_path}\")\n",
        "\n",
        "with open(history_path, 'wb') as f:\n",
        "    pickle.dump((train_loss_history, val_loss_history), f)\n",
        "print(f\"Training history saved at: {history_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkuzS6WFDq_r"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWBP8sPFAvba"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "\n",
        "for split in ['train', 'val']:\n",
        "  sample_idx = torch.randint(0, num_train if split=='train' else num_val, (batch_size,))\n",
        "  sample_images = process_images_batch([data_dict[split+'_images'][i] for i in sample_idx]\n",
        "                            , image_size=image_size, augment=False)\n",
        "  sample_captions = data_dict[split+'_captions'][sample_idx][:,random.randint(0, 4),:]\n",
        "\n",
        "  # decode_captions is loaded from a4_helper.py\n",
        "  gt_captions = decode_captions(sample_captions, data_dict['vocab']['idx_to_token'])\n",
        "  model.eval()\n",
        "  generated_captions, attn_weights_all = model.sample(sample_images)\n",
        "  generated_captions = decode_captions(generated_captions, data_dict['vocab']['idx_to_token'])\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    plt.imshow(sample_images[i].permute(1, 2, 0)/255)\n",
        "    plt.axis('off')\n",
        "    plt.title('%s\\nAttention LSTM Generated:%s\\nGT:%s' % (split, generated_captions[i], gt_captions[i]))\n",
        "    plt.show()\n",
        "\n",
        "    tokens = generated_captions[i].split(' ')\n",
        "\n",
        "    vis_attn = []\n",
        "    for j in range(len(tokens)):\n",
        "      img = sample_images[i]\n",
        "      attn_weights = attn_weights_all[i][j]\n",
        "      token = tokens[j]\n",
        "      img_copy = attention_visualizer(img, attn_weights, token)\n",
        "      vis_attn.append(transforms.ToTensor()(img_copy))\n",
        "\n",
        "    plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
        "    vis_attn = make_grid(vis_attn, nrow=8)\n",
        "    plt.imshow(torch.flip(vis_attn, dims=(0,)).permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.rcParams['figure.figsize'] = (10.0, 8.0)"
      ],
      "metadata": {
        "id": "5nAqufbs9W3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}